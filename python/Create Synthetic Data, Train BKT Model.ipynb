{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Notebook\n",
    "\n",
    "* Theory of Bayesian Knowledge Tracing (BKT)\n",
    "* Generating Synthetic Student Data According to BKT Assumptions\n",
    "* Generating Synthetic Student Data According to IRT Assumptions\n",
    "* Python Code for Generating Synthetic Data According to BKT Assumptions\n",
    "* Fitting an HMM to Synthetic Student Data\n",
    " * The Label Problem\n",
    "* Python Code for Fitting an HMM to Synethetic Student Data\n",
    "* Comparison with Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of Bayesian Knowledge Tracing (BKT)\n",
    "\n",
    "## Inference\n",
    "\n",
    "The probability of a student getting a question correct at time opportunity $n$ is:\n",
    "\n",
    "$$P(\\text{correct}_n) = P(L_n) \\cdot (1 - P(S)) + (1 - P(L_n)) \\cdot P(G)$$\n",
    "\n",
    "The probability of a student getting a question incorrect at time opportunity $n$ is:\n",
    "\n",
    "$$P(\\text{incorrect}_n) = P(L_n) \\cdot P(S) + (1 - P(L_n)) \\cdot (1 - P(G))$$\n",
    "\n",
    "## Fitting the Model: Part 1\n",
    "\n",
    "There are two distinct stages to using the knowledge tracing model. The first stage is to fit the model parameters from data. The second stage is using the model to infer the student’s knowledge over time given the student’s responses. Given an observation of the student’s response at time opportunity n (correct or incorrect) the probability\n",
    "P(Ln) that a student knows the skill is calculated using Bayes rule. When a correct response is observed, this probability is as follows:\n",
    "\n",
    "$$P(L_n | \\text{correct}_n) = \\frac{P(L_n) \\cdot (1 - P(S))}{P(\\text{correct}_n)}$$\n",
    "\n",
    "When an incorrect response is observed, this probability is as follows:\n",
    "\n",
    "$$P(L_n | \\text{incorrect}_n) = \\frac{P(L_n) \\cdot P(S)}{P(\\text{incorrect}_n)}$$\n",
    "\n",
    "## Fitting the Model: Part 2\n",
    "\n",
    "Lastly, we show how the student’s knowledge of the skill is updated given its interaction with the system. This estimate is the sum of two probabilities: the posterior probability that the student already knew the skill (contingent on the evidence), and the probability that the student did not know the skill, but was able to learn it.\n",
    "\n",
    "$$P(L_n) = P(L_{n-1} | \\text{evidence}_{n-1}) + (1 - P(L_{n-1} | \\text{evidence}_{n-1})) \\cdot P(T)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data According to BKT Assumptions\n",
    "\n",
    "To generate student data according to BKT assumptions, we start by deciding on the number of concepts that our synthetic student will learn from. In our running example, the students will answer questions on a single topic. Next, we set the true values for the parameters $P(L_0)$, $P(T)$, $P(G)$, and $P(S)$. Here, we choose $P(L_0) = 0.1$, $P(T) = 0.2$, $P(G) = 0.25$, and $P(S) = 0.1$. (If we were dealing with multiple concepts, we would choose a different set of values for each concept.) Finally, once we have the parameters, we generate student responses on a sequence of questions; in this example, 20 questions on a single topic.\n",
    "\n",
    "We generate student data in thre steps.\n",
    "\n",
    "1. Determine the probability of getting a question correct with the formula $P(\\text{correct}_n) = P(L_n) \\cdot (1 - P(S)) + (1 - P(L_n)) \\cdot P(G)$\n",
    "1. Draw an answer from a Bernoulli distribution with mean = $P(\\text{correct}_n)$.\n",
    "1. Update the probability of knowing the concept depending on whether the answer was correct or incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import csv\n",
    "from hmmlearn import hmm\n",
    "\n",
    "def generate_bkt_responses(numStudents, numQuestions, pL0=0.05, pT=0.1, pGuess=0.25, pSlip=0.05):\n",
    "    \n",
    "    all_answers = []\n",
    "    for i in range(numStudents):\n",
    "        pL = np.zeros(numQuestions)\n",
    "        pL[0] = pL0\n",
    "        answers = np.zeros(numQuestions)\n",
    "        for i in range(numQuestions):\n",
    "\n",
    "            # Answer ith question according to probability of knowing the concept at step i\n",
    "            pCorrect = (pL[i] * (1 - pSlip)) + ((1 - pL[i]) * pGuess)\n",
    "            cur_answer = np.random.binomial(1, pCorrect)\n",
    "            answers[i] = cur_answer\n",
    "\n",
    "            if i == numQuestions-1: break\n",
    "\n",
    "            # Update the probability of knowing the concept conditioned on whether the answer\n",
    "            # was correct or incorrect. \n",
    "            if cur_answer == 1:\n",
    "                numerator = pL[i] * (1 - pSlip)\n",
    "                denominator = numerator + ((1 - pL[i]) * pGuess)\n",
    "                pL[i+1] = (numerator / denominator)\n",
    "            elif cur_answer == 0:\n",
    "                numerator = pL[i] * pSlip\n",
    "                denominator = numerator + ((1 - pL[i]) * (1 - pGuess))\n",
    "                pL[i+1] = (numerator / denominator)\n",
    "\n",
    "            # Finally, update the probability of knowing the concept given the response\n",
    "            pL[i+1] = pL[i+1] + ((1 - pL[i+1]) * pT)\n",
    "       \n",
    "        answers = [int(a) for a in answers]\n",
    "        all_answers.append(answers)\n",
    "    return all_answers\n",
    "\n",
    "def print_bkt_params(hmm_fit):\n",
    "    print \"pT is: %f\" % hmm_fit.transmat_[0,1]\n",
    "    print \"pGuess is: %f\" % hmm_fit.emissionprob_[0,1]\n",
    "    print \"pSlip is: %f\" % hmm_fit.emissionprob_[1,0]\n",
    "    print \"pL0 is: %f\" % hmm_fit.startprob_[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Student Data According to IRT Assumptions\n",
    "\n",
    "\n",
    "Simulated Data: We simulate virtual students learning virtual concepts and test how well we can\n",
    "predict responses in this controlled setting. For each run of this experiment we generate two thousand students who answer 50 exercises drawn from $k$ concepts. For this dataset only, all students answer the same sequence of 50 exercises. Each student has a latent knowledge state \"skill\" for each concept, and each exercise has both a single concept and a difficulty. The probability of a student getting a exercise with difficulty β correct if the student had concept skill α is modelled using classic Item Response Theory as: \n",
    "\n",
    "$$P(\\text{correct} | \\alpha, \\beta) = c + \\frac{1-c}{1 + e^{\\beta - \\alpha}}$$\n",
    "\n",
    "where $c$ is the probability of a random guess, set to be 0.25. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting an HMM to Synthetic Student Data\n",
    "\n",
    "Once we have generated synthetic student data, we can re-learn the parameters used to generate the data using an HMM. Here we train a multinomial HMM using the Python package HMMLearn.\n",
    "\n",
    "We will learn the following sets of parameters:\n",
    "\n",
    "* Emission probabilities\n",
    "* Transmission probabilities\n",
    "* Start state probability\n",
    "\n",
    "Which will take the following form:\n",
    "\n",
    "![title](images/bkt_params.png)\n",
    "\n",
    "## The Label Switching Problem\n",
    "There is something called the Label-Switching Problem that we need to be careful of. Essentially, the rows and columns of the output probability matrices can be in any order. For example, there is no guarantee that the 0th row of the emission probability matrix corresponds to the 0th state. This means that we need a heuristic for determining the row and column labels after we learn HMM parameters.\n",
    "\n",
    "More info here:\n",
    "* https://github.com/hmmlearn/hmmlearn/issues/106\n",
    "* http://stackoverflow.com/questions/39756006/how-to-map-hidden-states-to-their-corresponding-categories-after-decoding-in-hmm\n",
    "* https://www.google.com/#q=label-switching+problem\n",
    "\n",
    "## The Label Switching Problem: Not a Problem in Matlab\n",
    "It appears that the label-switching problem does not occur in Matlab. No matter how I generate synthetic data (either in Python or Matlab), Matlab consistently outputs the parameter matrices in the same order. This is a strong reason to consider switching to Matlab for BKT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true value of pT is 0.20. The transmission matrix probability is: \n",
      "[[ 0.61733849  0.38266151]\n",
      " [ 0.16558054  0.83441946]]\n",
      "--------------------\n",
      "The true value of pG is 0.25.\n",
      "The true value of pS is 0.05. THe emission probability matrix is: \n",
      "[[ 0.55118856  0.44881144]\n",
      " [ 0.02975853  0.97024147]]\n",
      "--------------------\n",
      "The true value of pL0 is 0.10. The start probability matrix is:  \n",
      "[ 0.98676419  0.01323581]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Fit BKT parameters using synthetic student data\n",
    "\n",
    "# Set true parameter values for generating synthetic data\n",
    "pL0=0.1\n",
    "pT=0.2\n",
    "pG=0.25\n",
    "pS=0.05\n",
    "\n",
    "# Generate synthetic data and format correctly\n",
    "numStudents = 100\n",
    "numQuestions = 20\n",
    "answers = generate_bkt_responses(numStudents, numQuestions, pL0=pL0, pT=pT, pGuess=pG, pSlip=pS)\n",
    "lengths = [len(i) for i in answers]\n",
    "X = np.concatenate(answers)\n",
    "X = np.atleast_2d(X).T\n",
    "\n",
    "# Learn parameters from synthetic data\n",
    "bkt_model = hmm.MultinomialHMM(n_components=2)\n",
    "bkt_fit = bkt_model.fit(X, lengths)\n",
    "\n",
    "print \"The true value of pT is %.02f. The transmission matrix probability is: \" % pT\n",
    "print bkt_fit.transmat_\n",
    "print \"--------------------\"\n",
    "print \"The true value of pG is %.02f.\" % pG\n",
    "print \"The true value of pS is %.02f. THe emission probability matrix is: \" % pS\n",
    "print bkt_fit.emissionprob_\n",
    "print \"--------------------\"\n",
    "print \"The true value of pL0 is %.02f. The start probability matrix is:  \" % pL0\n",
    "print bkt_fit.startprob_\n",
    "print \"--------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data in Matlab and Train Parameters in Python\n",
    "\n",
    "It is simple to specify the parameters for an HMM in Matlab and generate sequences. In order to validate the code I wrote here to learn HMM params, I created an HMM with the same \"true\" paramters as here in Matlab and generated synthetic student data. Below I read in that data and learn the parameters using Python.\n",
    "\n",
    "Currently, the parameters used to generate data in Python are the same as those used to generate data in Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transmission matrix probability is: \n",
      "[[ 0.52290758  0.47709242]\n",
      " [ 0.50307994  0.49692006]]\n",
      "--------------------\n",
      "The emission probability matrix is: \n",
      "[[ 0.13947199  0.86052801]\n",
      " [ 0.22456961  0.77543039]]\n",
      "--------------------\n",
      "The start probability matrix is: \n",
      "[ 0.24379197  0.75620803]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Read in data generated from an HMM in Matlab. \n",
    "# Currently, the parameters used in Matlab are the same as those used here.\n",
    "matlab_file = '../data/matlab_synthetic.csv'\n",
    "answers = []\n",
    "lengths = []\n",
    "with open(matlab_file, 'rb') as csvfile:\n",
    "    data_reader = csv.reader(csvfile)\n",
    "    for row in data_reader:\n",
    "        new_row = [int(i)-1 for i in row]\n",
    "        answers.append(new_row)\n",
    "        lengths.append(len(row))\n",
    "X = np.concatenate(answers)\n",
    "X = np.atleast_2d(X).T\n",
    "\n",
    "# Initial guesses (prior beliefs) of HMM parameters\n",
    "trans = np.array([[0.6,0.4],\n",
    "                  [0,1]])\n",
    "start = np.array([0.9, 0.1])\n",
    "# For some reason, the emission probabilities can't be specified in HMMLearn (but they can in Matlab)\n",
    "\n",
    "bkt_model = hmm.MultinomialHMM(n_components=2, transmat_prior=trans, startprob_prior=start)\n",
    "bkt_fit = bkt_model.fit(X, lengths, )\n",
    "\n",
    "print \"The transmission matrix probability is: \"\n",
    "print bkt_fit.transmat_\n",
    "print \"--------------------\"\n",
    "print \"The emission probability matrix is: \"\n",
    "print bkt_fit.emissionprob_\n",
    "print \"--------------------\"\n",
    "print \"The start probability matrix is: \"\n",
    "print bkt_fit.startprob_\n",
    "print \"--------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data in Python and Train Parameters in Matlab\n",
    "\n",
    "In order to validate the Python method I used generate synthetic student data (which should be the same as the Matlab method), I write the synthetic student data to a csv file, load it in Matlab, and learn the HMM params using Matlab's HMM toolbox.\n",
    "\n",
    "Currently, the parameters used to generate data in Python are the same as those used to generate data in Matlab (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write answers to CSV and learn parameters in Matlab\n",
    "with open('../data/python_synthetic.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for a in answers:\n",
    "        newrow = [i+1 for i in a]\n",
    "        writer.writerow(newrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matlab's Learned Params\n",
    "\n",
    "Matlab always outputs the transmission and emission probability matrices in the same order. Here is one example:\n",
    "\n",
    "    estTR =\n",
    "\n",
    "    0.8022    0.1978\n",
    "         0    1.0000\n",
    "\n",
    "\n",
    "    estE =\n",
    "\n",
    "    0.7733    0.2267\n",
    "    0.0515    0.9485\n",
    "    \n",
    "The true values used to generate the data in Python were pL0=0.1, pT=0.2, pG=0.25, and pS=0.05. Here we see those values are estimated by Matlab to be pT-hat=0.20, pG-hat=0.23, and pS-hat=0.05. Note that Matlab does not return pL0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
